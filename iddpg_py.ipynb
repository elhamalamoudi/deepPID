{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iddpg.py",
      "provenance": [],
      "authorship_tag": "ABX9TyPrRI4zy/nY2tkpryBVRfsR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamalamoudi/deepPID/blob/master/iddpg_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvWE-uPp1V0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "21115740-980c-44bf-cd08-b8c5cf4c5e54"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "from ou_noise import OUNoise\n",
        "\n",
        "LAYER_1 = 400\n",
        "LAYER_2 = 300\n",
        "LAYER_3 = 300\n",
        "keep_rate = 0.8\n",
        "LAMBDA = 0.00001 # regularization term\n",
        "GAMMA = 0.99\n",
        "class IDDPG(object):\n",
        "\n",
        "\n",
        "    def __init__(self, sess, state_dim, action_dim, max_action, min_action, actor_learning_rate, critic_learning_rate, tau, RANDOM_SEED, device = '/cpu:0'):\n",
        "\n",
        "        self.sess = sess\n",
        "        np.random.seed(RANDOM_SEED)\n",
        "        tf.set_random_seed(RANDOM_SEED)\n",
        "        self.s_dim = state_dim\n",
        "        self.a_dim = action_dim\n",
        "        self.actor_learning_rate = actor_learning_rate\n",
        "        self.critic_learning_rate = critic_learning_rate\n",
        "        self.tau = tau\n",
        "        self.device = device\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        # Placeholders\n",
        "        self.inputs = tf.placeholder(tf.float32, shape=[None, self.s_dim], name='state')\n",
        "        self.action = tf.placeholder(tf.float32, shape=[None, self.a_dim], name='actions')\n",
        "        scope = 'net'    \n",
        "        self.v, self.a, self.scaled_a, self.saver = self._build_net(scope)\n",
        "        self.a_params = tf.trainable_variables(scope=scope + '/actor')\n",
        "        self.c_params = tf.trainable_variables(scope=scope + '/critic')\n",
        "        #self.a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
        "        #self.c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
        "        scope = 'target'    \n",
        "        self.v_target, self.a_target, self.scaled_a_target, self.saver_target = self._build_net(scope)\n",
        "        self.a_params_target = tf.trainable_variables(scope=scope + '/actor')\n",
        "        self.c_params_target = tf.trainable_variables(scope=scope + '/critic')\n",
        "        #self.a_params_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
        "        #self.c_params_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
        "\n",
        "        \n",
        "        with tf.variable_scope('learning_rate'): \n",
        "            # global step\n",
        "            self.global_step = tf.Variable(0, trainable=False)\n",
        "            self.actor_decay_learning_rate = tf.train.exponential_decay(self.actor_learning_rate, self.global_step, 100000, 0.96, staircase=True)\n",
        "            self.critic_decay_learning_rate = tf.train.exponential_decay(self.critic_learning_rate, self.global_step, 100000, 0.96, staircase=True)\n",
        "        \n",
        "        with tf.device(self.device):\n",
        "            # Op for periodically updating target network with online network\n",
        "            # weights with regularization\n",
        "            self.generate_param_updater()\n",
        "           \n",
        "            self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
        "            # Define loss and optimization Op\n",
        "            self.squared = tf.square(tf.subtract(self.predicted_q_value,self.v))\n",
        "            self.l2_loss = tf.losses.get_regularization_loss(scope=\"net/critic\")\n",
        "            self.loss = tf.reduce_mean(self.squared) + self.l2_loss \n",
        "            self.critic_optimize = tf.train.AdamOptimizer(self.critic_decay_learning_rate).minimize(self.loss, global_step=self.global_step) \n",
        "            self.action_grads = tf.gradients(self.v, self.action)[0]\n",
        "            self.actor_gradients = tf.gradients(self.a, self.a_params, -self.action_grads)\n",
        "            self.actor_optimize = tf.train.AdamOptimizer(self.actor_decay_learning_rate).apply_gradients(zip(self.actor_gradients, self.a_params), global_step=self.global_step)\n",
        "            \n",
        "            # inverting gradients\n",
        "            self.inverting_gradients_placeholder = tf.placeholder(tf.float32, shape=[None, self.a_dim], name='inverting_gradients')\n",
        "            self._dq_da = tf.gradients(self.v, self.action)[0] # q, a \n",
        "            self._grad = tf.gradients(self.a, self.a_params, -self.inverting_gradients_placeholder)\n",
        "            self._train_actor = tf.train.AdamOptimizer(self.actor_decay_learning_rate).apply_gradients(zip(self._grad, self.a_params),global_step=self.global_step)\n",
        "            \n",
        "            \n",
        "\n",
        "\n",
        "    def _build_net(self,scope):\n",
        "       \n",
        "        with tf.device(self.device):        \n",
        "            with tf.variable_scope(scope + '/critic'):\n",
        "                \n",
        "                \n",
        "                '''\n",
        "                net = tf.layers.dense(self.inputs, LAYER_1, tf.nn.relu, name='critic_L1')\n",
        "                initializer = tf.variance_scaling_initializer()\n",
        "                s_union_weights = tf.Variable(initializer.__call__([LAYER_1, LAYER_2]), name='critic_L2_Ws')\n",
        "                a_union_weights = tf.Variable(initializer.__call__([self.a_dim, LAYER_2]), name='critic_L2_Wa')\n",
        "                union_biases = tf.Variable(tf.zeros([LAYER_2]), name='critic_L2_b')\n",
        "                net = tf.nn.relu(tf.matmul(net, s_union_weights) + tf.matmul(self.action, a_union_weights) + union_biases,name='critic_L2')\n",
        "                w_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "                v = tf.layers.dense(net, self.a_dim, kernel_initializer=w_init, name='critic_output')\n",
        "                '''\n",
        "                regularizer = tf.contrib.layers.l2_regularizer(scale=LAMBDA)\n",
        "                l1 = tf.contrib.layers.fully_connected(self.inputs, LAYER_1, weights_regularizer=regularizer, activation_fn=tf.nn.leaky_relu)\n",
        "                l2_a = tf.contrib.layers.fully_connected(self.action, LAYER_2, weights_regularizer=regularizer, activation_fn=None)\n",
        "                l2_s = tf.contrib.layers.fully_connected(l1, LAYER_2, weights_regularizer=regularizer,activation_fn=None)\n",
        "                l2 = tf.nn.leaky_relu(l2_s + l2_a)\n",
        "                v = tf.contrib.layers.fully_connected(l2, 1, weights_regularizer=regularizer, activation_fn=None)\n",
        "\t\t\t\t\n",
        "            with tf.variable_scope(scope + '/actor'):\n",
        "                l1 = tf.contrib.layers.fully_connected(self.inputs, LAYER_1,  activation_fn=tf.nn.leaky_relu) # tf.nn.leaky_relu tf.nn.relu\n",
        "                l2 = tf.contrib.layers.fully_connected(l1, LAYER_2,  activation_fn=tf.nn.leaky_relu)\n",
        "                w_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "                a = tf.contrib.layers.fully_connected(l2, self.a_dim, weights_initializer=w_init, activation_fn=None) # None  tf.nn.tanh\n",
        "                scaled_a = a\n",
        "                # scaled_a = tf.clip_by_value(a,self.min_action,self.max_action)#tf.multiply(a, self.action_bound)\n",
        "                       \n",
        "        saver = tf.train.Saver()\n",
        "        return v, a, scaled_a, saver\n",
        "\n",
        "    def train(self, s_batch, a_batch, r_batch, t_batch, s2_batch, MINIBATCH_SIZE):\n",
        "        \n",
        "        \n",
        "        # get q target\n",
        "        target_q = self.critic_predict_target(s2_batch, self.predict_action_target(s2_batch))\n",
        "        # obtain y\n",
        "        y_i = []\n",
        "        for k in range(MINIBATCH_SIZE):\n",
        "            if t_batch[k]:\n",
        "                y_i.append(r_batch[k])\n",
        "            else:\n",
        "                y_i.append(r_batch[k] + GAMMA * target_q[k])\n",
        "        \n",
        "        # train critic\n",
        "        LOSS = self.critic_train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)))\n",
        "        # train critic\n",
        "        #ac_tor_grads = self._critic_train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)))\n",
        "        #print('a grads',ac_tor_grads)\n",
        "        actions = self.predict_action(s_batch)\n",
        "        \n",
        "        upper = self.max_action\n",
        "        lower = self.min_action\n",
        "\n",
        "        # get dq/da array, action array\n",
        "        #print(upper, '***************')\n",
        "        dq_das = self.sess.run([self._dq_da], feed_dict={self.inputs: s_batch, self.action:actions})[0]\n",
        "        # inverting gradients, if dq_da >= 0, apply upper method, else lower method\n",
        "        inverting_gradients = []\n",
        "        #'''\n",
        "        # print('1 dq_das, actions',dq_das, actions)\n",
        "        '''\n",
        "        # print('dq_das, actions',dq_das, actions)\n",
        "        for dq_da, action in zip(dq_das, actions):\n",
        "            # print('dq_da, action',dq_da, action)\n",
        "            if dq_da >= 0.0:\n",
        "                inverting_gradients.append(dq_da * (self.max_action - action) / (self.max_action - self.min_action))\n",
        "            else:\n",
        "                inverting_gradients.append(dq_da * (action - self.min_action) / (self.max_action - self.min_action))\n",
        "        inverting_gradients = np.array(inverting_gradients).reshape(-1, 1)\n",
        "\t\t'''\n",
        "\n",
        "        for i in range(MINIBATCH_SIZE):\n",
        "            #print('2', i,dq_das[i])\n",
        "            for j in range(self.a_dim):\n",
        "                if dq_das[i][j] >= 0.0:\n",
        "                    dq_das[i][j] = dq_das[i][j] * (self.max_action - actions[i][j]) / (self.max_action - self.min_action)\n",
        "                else:\n",
        "                    dq_das[i][j] = dq_das[i][j] * (actions[i][j] - self.min_action) / (self.max_action - self.min_action)\n",
        "        \n",
        "        # print(dq_das,inverting_gradients)\n",
        "        # exit()\n",
        "        inverting_gradients = dq_das \n",
        "        \n",
        "        # print('2 dq_das, actions',dq_das, actions)\n",
        "        \n",
        "         \n",
        "        #print('1','inverting_gradients',inverting_gradients)\n",
        "        \n",
        "        # print('2','inverting_gradients',inverting_gradients,dq_das, actions)\n",
        "        # time.sleep(1)\n",
        "        # update actor\n",
        "        self.sess.run(self._train_actor, feed_dict={self.inputs: s_batch, self.inverting_gradients_placeholder: inverting_gradients})\n",
        "        self.update_target_network()\n",
        "        return\n",
        "\n",
        "    \n",
        "    def _critic_train(self, inputs, action, predicted_q_value):\n",
        "        return self.sess.run([self.action_grads], feed_dict={\n",
        "            self.inputs: inputs,\n",
        "            self.action: action,\n",
        "            self.predicted_q_value: predicted_q_value\n",
        "        })\n",
        "\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.sess.run([self.a_updater,self.c_updater])\n",
        "\n",
        "    def generate_param_updater(self):\n",
        "        self.a_updater = [self.a_params_target[i].assign(tf.multiply(self.a_params[i], self.tau) + tf.multiply(self.a_params_target[i], 1. - self.tau))\n",
        "                for i in range(len(self.a_params))]\n",
        "        self.c_updater = [self.c_params_target[i].assign(tf.multiply(self.c_params[i], self.tau) + tf.multiply(self.c_params_target[i], 1. - self.tau))\n",
        "                for i in range(len(self.c_params))]\n",
        "\n",
        "    def critic_train(self, inputs, action, predicted_q_value):\n",
        "        return self.sess.run([self.loss,self.critic_optimize], feed_dict={\n",
        "            self.inputs: inputs,\n",
        "            self.action: action,\n",
        "            self.predicted_q_value: predicted_q_value\n",
        "        })\n",
        "\n",
        "\n",
        "    def actor_train(self,inputs, action):\n",
        "        return self.sess.run(self.actor_optimize, feed_dict={\n",
        "            self.inputs: inputs,\n",
        "            self.action: action\n",
        "        })\n",
        "\n",
        "    def save(self):\n",
        "        self.saver.save(self.sess,\"./model/model.ckpt\")\n",
        "        self.saver_target.save(self.sess,\"./model/model_target.ckpt\")\n",
        "        print(\"Model saved in file: actor_model\")\n",
        "\n",
        "    \n",
        "    def load(self):\n",
        "        self.saver.restore(self.sess,\"./model/model.ckpt\")\n",
        "        self.saver_target.restore(self.sess,\"./model/model_target.ckpt\")\n",
        "        \n",
        "\n",
        "\n",
        "    def critic_predict_target(self, state, action):\n",
        "        return self.sess.run(self.v_target, feed_dict={\n",
        "            self.inputs: state,\n",
        "            self.action: action\n",
        "        })\n",
        "        \n",
        "    def predict_action_target(self, state):\n",
        "        return self.sess.run(self.scaled_a_target, feed_dict={\n",
        "            self.inputs: state\n",
        "        })\n",
        "\n",
        "    def predict_action(self, state):\n",
        "        return self.sess.run(self.scaled_a, feed_dict={\n",
        "            self.inputs: state\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from replay_buffer import ReplayBuffer\n",
        "    ACTOR_LEARNING_RATE = 0.0001\n",
        "    CRITIC_LEARNING_RATE =  0.001\n",
        "    # Soft target update param\n",
        "    TAU = 0.001\n",
        "    DEVICE = '/cpu:0'\n",
        "    # ENV_NAME = 'MountainCarContinuous-v0'\n",
        "    ENV_NAME = 'Pendulum-v0'\n",
        "    # import gym_foo\n",
        "    # ENV_NAME = 'nessie_end_to_end-v0'\n",
        "    max_action = 2.\n",
        "    min_action = -2.\n",
        "    epochs = 500\n",
        "    epsilon = 1.0\n",
        "    min_epsilon = 0.1\n",
        "    EXPLORE = 200\n",
        "    BUFFER_SIZE = 100000\n",
        "    RANDOM_SEED = 51234\n",
        "    MINIBATCH_SIZE = 64# 32 # 5\n",
        "    with tf.Session() as sess:\n",
        "        np.random.seed(RANDOM_SEED)\n",
        "        tf.set_random_seed(RANDOM_SEED)\n",
        "        env = gym.make(ENV_NAME)\n",
        "        state_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.shape[0]\n",
        "        low = DDPG(sess, state_dim, action_dim, max_action, min_action, ACTOR_LEARNING_RATE, CRITIC_LEARNING_RATE, TAU, RANDOM_SEED,device=DEVICE)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        # check that we are effectively updating the parameters\n",
        "        #print(low.a_params_target[0].eval()[0][0],low.a_params[0].eval()[0][0])\n",
        "        #low.update_target_network()\n",
        "        #print(low.a_params_target[0].eval()[0][0],low.a_params[0].eval()[0][0])\n",
        "        replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
        "        ruido = OUNoise(action_dim, mu = 0.0)\n",
        "        for i in range(epochs):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            epsilon -= (epsilon/EXPLORE)\n",
        "            epsilon = np.maximum(min_epsilon,epsilon)\n",
        "            episode_r = 0.\n",
        "            step = 0\n",
        "            while (not done):\n",
        "                step += 1\n",
        "                action = low.predict_action(np.reshape(state,(1,state_dim)))\n",
        "                action1 = action\n",
        "                action = np.clip(action,min_action,max_action)\n",
        "                action = action + max(epsilon,0)*ruido.noise()\n",
        "                action = np.clip(action,min_action,max_action)\n",
        "                # print(action1, action)\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                reward = reward + 1. \n",
        "                # reward = np.clip(reward,-1.,1.)\n",
        "                replay_buffer.add(np.reshape(state, (state_dim,)), np.reshape(action, (action_dim,)), reward,\n",
        "                                      done, np.reshape(next_state, (state_dim,)))\n",
        "                state = next_state\n",
        "                episode_r = episode_r + reward\n",
        "                if replay_buffer.size() > MINIBATCH_SIZE:\n",
        "                    s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(MINIBATCH_SIZE)\n",
        "                    low.train(s_batch, a_batch, r_batch, t_batch, s2_batch,MINIBATCH_SIZE)\n",
        "                   \n",
        "            print(i, step, 'last r', round(reward,3), 'episode reward',round(episode_r,3), 'epsilon', round(epsilon,3))                \n",
        "\n",
        "\n",
        "        low.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-802cd1350731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mou_noise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOUNoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mLAYER_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ou_noise'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}