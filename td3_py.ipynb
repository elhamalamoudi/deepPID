{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "td3.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvWtnMSO00m0DGzjGxmaqX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamalamoudi/deepPID/blob/master/td3_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvWt3h9dHngY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "8888dc49-bcb7-40a3-9ac3-67688f9ce160"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "from ou_noise import OUNoise\n",
        "\n",
        "LAYER_1 = 400\n",
        "LAYER_2 = 300\n",
        "LAYER_3 = 300\n",
        "keep_rate = 0.8\n",
        "LAMBDA = 0.00001 # regularization term\n",
        "GAMMA = 0.99\n",
        "class TD3(object):\n",
        "\n",
        "\n",
        "    def __init__(self, sess, state_dim, action_dim, max_action, min_action, actor_learning_rate, critic_learning_rate, tau, RANDOM_SEED, device = '/cpu:0'):\n",
        "\n",
        "        self.sess = sess\n",
        "        np.random.seed(RANDOM_SEED)\n",
        "    \ttf.set_random_seed(RANDOM_SEED)\n",
        "        self.s_dim = state_dim\n",
        "        self.a_dim = action_dim\n",
        "        self.actor_learning_rate = actor_learning_rate\n",
        "        self.critic_learning_rate = critic_learning_rate\n",
        "        self.tau = tau\n",
        "        self.device = device\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.count = 0 # count for \n",
        "        # Placeholders\n",
        "        self.inputs = tf.placeholder(tf.float32, shape=[None, self.s_dim], name='state')\n",
        "        self.action = tf.placeholder(tf.float32, shape=[None, self.a_dim], name='actions')\n",
        "        scope = 'net'    \n",
        "        self.v_1,self.v_2, self.a, self.scaled_a, self.saver = self._build_net(scope)\n",
        "        self.a_params = tf.trainable_variables(scope=scope + '/actor')\n",
        "        self.c_params_1 = tf.trainable_variables(scope=scope + '/critic_1')\n",
        "        self.c_params_2 = tf.trainable_variables(scope=scope + '/critic_2')\n",
        "        #self.a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
        "        #self.c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
        "        scope = 'target'    \n",
        "        self.v_target_1, self.v_target_2, self.a_target, self.scaled_a_target, self.saver_target = self._build_net(scope)\n",
        "        self.a_params_target = tf.trainable_variables(scope=scope + '/actor')\n",
        "        self.c_params_target_1 = tf.trainable_variables(scope=scope + '/critic_1')\n",
        "        self.c_params_target_2 = tf.trainable_variables(scope=scope + '/critic_2')\n",
        "        #self.a_params_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
        "        #self.c_params_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
        "\n",
        "        \n",
        "        with tf.variable_scope('learning_rate'): \n",
        "            # global step\n",
        "            self.global_step = tf.Variable(0, trainable=False)\n",
        "            self.actor_decay_learning_rate = tf.train.exponential_decay(self.actor_learning_rate, self.global_step, 100000, 0.96, staircase=True)\n",
        "            self.critic_decay_learning_rate = tf.train.exponential_decay(self.critic_learning_rate, self.global_step, 100000, 0.96, staircase=True)\n",
        "        \n",
        "        with tf.device(self.device):\n",
        "            # Op for periodically updating target network with online network\n",
        "            # weights with regularization\n",
        "            self.generate_param_updater()\n",
        "           \n",
        "            self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
        "            # Define loss and optimization Op\n",
        "            self.v_target_min = tf.minimum(self.v_target_1,self.v_target_2)\n",
        "            \n",
        "            self.squared_1 = tf.square(tf.subtract(self.predicted_q_value,self.v_1))\n",
        "            self.l2_loss_1 = tf.losses.get_regularization_loss(scope=\"net/critic_1\")\n",
        "            self.loss_1 = tf.reduce_mean(self.squared_1) + self.l2_loss_1 \n",
        "            self.squared_2 = tf.square(tf.subtract(self.predicted_q_value,self.v_2))\n",
        "            self.l2_loss_2 = tf.losses.get_regularization_loss(scope=\"net/critic_2\")\n",
        "            self.loss_2 = tf.reduce_mean(self.squared_2) + self.l2_loss_2\n",
        "            self.loss = self.loss_1 + self.loss_2\n",
        "            self.critic_optimize = tf.train.AdamOptimizer(self.critic_learning_rate).minimize(self.loss, global_step=self.global_step) \n",
        "            \n",
        "\n",
        "            self.action_grads = tf.gradients(self.v_1, self.action)[0]\n",
        "            self.actor_gradients = tf.gradients(self.a, self.a_params, -self.action_grads)\n",
        "            self.actor_optimize = tf.train.AdamOptimizer(self.actor_learning_rate).apply_gradients(zip(self.actor_gradients, self.a_params), global_step=self.global_step)\n",
        "            \n",
        "            # inverting gradients\n",
        "            self.inverting_gradients_placeholder = tf.placeholder(tf.float32, shape=[None, self.a_dim], name='inverting_gradients')\n",
        "            self._dq_da = tf.gradients(self.v_1, self.action)[0] # q, a \n",
        "            self._grad = tf.gradients(self.a, self.a_params, -self.inverting_gradients_placeholder)\n",
        "            self._train_actor = tf.train.AdamOptimizer(self.actor_learning_rate).apply_gradients(zip(self._grad, self.a_params),global_step=self.global_step)\n",
        "            \n",
        "            \n",
        "\n",
        "\n",
        "    def _build_net(self,scope):\n",
        "       \n",
        "        with tf.device(self.device):        \n",
        "            with tf.variable_scope(scope + '/critic_1'):\n",
        "\n",
        "                regularizer = tf.contrib.layers.l2_regularizer(scale=LAMBDA)\n",
        "                l1 = tf.contrib.layers.fully_connected(self.inputs, LAYER_1, weights_regularizer=regularizer, activation_fn=tf.nn.leaky_relu)\n",
        "                l2_a = tf.contrib.layers.fully_connected(self.action, LAYER_2, weights_regularizer=regularizer, activation_fn=None)\n",
        "                l2_s = tf.contrib.layers.fully_connected(l1, LAYER_2, weights_regularizer=regularizer,activation_fn=None)\n",
        "                l2 = tf.nn.leaky_relu(l2_s + l2_a)\n",
        "                v_1 = tf.contrib.layers.fully_connected(l2, 1, weights_regularizer=regularizer, activation_fn=None)\n",
        "\n",
        "            with tf.variable_scope(scope + '/critic_2'):\n",
        "                l1 = tf.contrib.layers.fully_connected(self.inputs, LAYER_1, weights_regularizer=regularizer, activation_fn=tf.nn.leaky_relu)\n",
        "                l2_a = tf.contrib.layers.fully_connected(self.action, LAYER_2, weights_regularizer=regularizer, activation_fn=None)\n",
        "                l2_s = tf.contrib.layers.fully_connected(l1, LAYER_2, weights_regularizer=regularizer,activation_fn=None)\n",
        "                l2 = tf.nn.leaky_relu(l2_s + l2_a)\n",
        "                v_2 = tf.contrib.layers.fully_connected(l2, 1, weights_regularizer=regularizer, activation_fn=None)\n",
        "\t\t\t\t\n",
        "            with tf.variable_scope(scope + '/actor'):\n",
        "                l1 = tf.contrib.layers.fully_connected(self.inputs, LAYER_1,  activation_fn=tf.nn.leaky_relu) # tf.nn.leaky_relu tf.nn.relu\n",
        "                l2 = tf.contrib.layers.fully_connected(l1, LAYER_2,  activation_fn=tf.nn.leaky_relu)\n",
        "                w_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "                a = tf.contrib.layers.fully_connected(l2, self.a_dim, weights_initializer=w_init, activation_fn=None) # None  tf.nn.tanh\n",
        "                scaled_a = a\n",
        "                # scaled_a = tf.clip_by_value(a,self.min_action,self.max_action)#tf.multiply(a, self.action_bound)\n",
        "                       \n",
        "        saver = tf.train.Saver()\n",
        "        return v_1, v_2, a, scaled_a, saver\n",
        "\n",
        "    def train_normal(self, s_batch, a_batch, r_batch, t_batch, s2_batch, MINIBATCH_SIZE):\n",
        "        \n",
        "        \n",
        "        # get q target\n",
        "        target_q = self.critic_predict_target(s2_batch, self.predict_action_target(s2_batch))\n",
        "        # obtain y\n",
        "        y_i = []\n",
        "        for k in range(MINIBATCH_SIZE):\n",
        "            if t_batch[k]:\n",
        "                y_i.append(r_batch[k])\n",
        "            else:\n",
        "                y_i.append(r_batch[k] + GAMMA * target_q[k])\n",
        "        # train critic\n",
        "        LOSS = self.critic_train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)))\n",
        "        # print(L2_LOSS)\n",
        "        a_outs = self.predict_action(s_batch)\n",
        "        self.actor_train(s_batch, a_outs)\n",
        "        \n",
        "        self.update_target_network()\n",
        "\n",
        "        return\n",
        "\n",
        "    def train(self, s_batch, a_batch, r_batch, t_batch, s2_batch, MINIBATCH_SIZE):\n",
        "        \n",
        "        \n",
        "        # get q target\n",
        "        target_q = self.critic_predict_target(s2_batch, self.predict_action_target(s2_batch))\n",
        "        # obtain y\n",
        "        y_i = []\n",
        "        for k in range(MINIBATCH_SIZE):\n",
        "            if t_batch[k]:\n",
        "                y_i.append(r_batch[k])\n",
        "            else:\n",
        "                y_i.append(r_batch[k] + GAMMA * target_q[k])\n",
        "        \n",
        "        # train critic\n",
        "        LOSS = self.critic_train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)))\n",
        "        \n",
        "        self.count +=1 \n",
        "        \n",
        "        if self.count == 2:\n",
        "\t        \n",
        "\t        self.count = 0\n",
        "\t        actions = self.predict_action(s_batch) \n",
        "\t        noise = np.clip(0.2*np.random.randn(MINIBATCH_SIZE,self.a_dim) + 0.,-0.5,0.5)\n",
        "\t        actions = np.clip(actions + noise, self.min_action,self.max_action)\n",
        "\t        \n",
        "\n",
        "\t        # get dq/da array, action array\n",
        "\t        #print(upper, '***************')\n",
        "\t        dq_das = self.sess.run([self._dq_da], feed_dict={self.inputs: s_batch, self.action:actions})[0]\n",
        "\t        # inverting gradients, if dq_da >= 0, apply upper method, else lower method\n",
        "\t        inverting_gradients = []\n",
        "\t        #'''\n",
        "\t        # print('1 dq_das, actions',dq_das, actions)\n",
        "\t        '''\n",
        "\t        # print('dq_das, actions',dq_das, actions)\n",
        "\t        for dq_da, action in zip(dq_das, actions):\n",
        "\t            # print('dq_da, action',dq_da, action)\n",
        "\t            if dq_da >= 0.0:\n",
        "\t                inverting_gradients.append(dq_da * (self.max_action - action) / (self.max_action - self.min_action))\n",
        "\t            else:\n",
        "\t                inverting_gradients.append(dq_da * (action - self.min_action) / (self.max_action - self.min_action))\n",
        "\t        inverting_gradients = np.array(inverting_gradients).reshape(-1, 1)\n",
        "\t\t\t'''\n",
        "\n",
        "\t        for i in range(MINIBATCH_SIZE):\n",
        "\t            #print('2', i,dq_das[i])\n",
        "\t            for j in range(self.a_dim):\n",
        "\t                if dq_das[i][j] >= 0.0:\n",
        "\t                    dq_das[i][j] = dq_das[i][j] * (self.max_action - actions[i][j]) / (self.max_action - self.min_action)\n",
        "\t                else:\n",
        "\t                    dq_das[i][j] = dq_das[i][j] * (actions[i][j] - self.min_action) / (self.max_action - self.min_action)\n",
        "\t        \n",
        "\t        # print(dq_das,inverting_gradients)\n",
        "\t        # exit()\n",
        "\t        inverting_gradients = dq_das \n",
        "\t       \n",
        "\t        self.sess.run(self._train_actor, feed_dict={self.inputs: s_batch, self.inverting_gradients_placeholder: inverting_gradients})\n",
        "\n",
        "        \tself.update_target_network()\n",
        "\n",
        "        return\n",
        "\n",
        "    \n",
        "    def _critic_train(self, inputs, action, predicted_q_value):\n",
        "        return self.sess.run([self.action_grads], feed_dict={\n",
        "            self.inputs: inputs,\n",
        "            self.action: action,\n",
        "            self.predicted_q_value: predicted_q_value\n",
        "        })\n",
        "\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.sess.run([self.a_updater,self.c_updater_1,self.c_updater_2])\n",
        "\n",
        "    def generate_param_updater(self):\n",
        "        self.a_updater = [self.a_params_target[i].assign(tf.multiply(self.a_params[i], self.tau) + tf.multiply(self.a_params_target[i], 1. - self.tau))\n",
        "                for i in range(len(self.a_params))]\n",
        "        self.c_updater_1 = [self.c_params_target_1[i].assign(tf.multiply(self.c_params_1[i], self.tau) + tf.multiply(self.c_params_target_1[i], 1. - self.tau))\n",
        "                for i in range(len(self.c_params_1))]\n",
        "        self.c_updater_2 = [self.c_params_target_2[i].assign(tf.multiply(self.c_params_2[i], self.tau) + tf.multiply(self.c_params_target_2[i], 1. - self.tau))\n",
        "                for i in range(len(self.c_params_2))]\n",
        "\n",
        "    def critic_train(self, inputs, action, predicted_q_value):\n",
        "        return self.sess.run([self.loss,self.critic_optimize], feed_dict={\n",
        "            self.inputs: inputs,\n",
        "            self.action: action,\n",
        "            self.predicted_q_value: predicted_q_value\n",
        "        })\n",
        "\n",
        "\n",
        "    def actor_train(self,inputs, action):\n",
        "        return self.sess.run(self.actor_optimize, feed_dict={\n",
        "            self.inputs: inputs,\n",
        "            self.action: action\n",
        "        })\n",
        "\n",
        "    def save(self):\n",
        "        self.saver.save(self.sess,\"./model/model.ckpt\")\n",
        "        self.saver_target.save(self.sess,\"./model/model_target.ckpt\")\n",
        "        print(\"Model saved in file: actor_model\")\n",
        "\n",
        "    \n",
        "    def load(self):\n",
        "        self.saver.restore(self.sess,\"./model/model.ckpt\")\n",
        "        self.saver_target.restore(self.sess,\"./model/model_target.ckpt\")\n",
        "        \n",
        "\n",
        "\n",
        "    def critic_predict_target(self, state, action):\n",
        "        return self.sess.run(self.v_target_min, feed_dict={\n",
        "            self.inputs: state,\n",
        "            self.action: action\n",
        "        })\n",
        "        \n",
        "    def predict_action_target(self, state):\n",
        "        return self.sess.run(self.scaled_a_target, feed_dict={\n",
        "            self.inputs: state\n",
        "        })\n",
        "\n",
        "    def predict_action(self, state):\n",
        "        return self.sess.run(self.scaled_a, feed_dict={\n",
        "            self.inputs: state\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from replay_buffer import ReplayBuffer\n",
        "    ACTOR_LEARNING_RATE = 0.0001\n",
        "    CRITIC_LEARNING_RATE =  0.0001\n",
        "    # Soft target update param\n",
        "    TAU = 0.005\n",
        "    DEVICE = '/cpu:0'\n",
        "    # ENV_NAME = 'MountainCarContinuous-v0'\n",
        "    ENV_NAME = 'Pendulum-v0'\n",
        "    # import gym_foo\n",
        "    # ENV_NAME = 'nessie_end_to_end-v0'\n",
        "    max_action = 2.\n",
        "    min_action = -2.\n",
        "    epochs = 500\n",
        "    epsilon = 1.0\n",
        "    min_epsilon = 0.1\n",
        "    EXPLORE = 200\n",
        "    BUFFER_SIZE = 100000\n",
        "    RANDOM_SEED = 51234\n",
        "    MINIBATCH_SIZE = 100# 32 # 5\n",
        "    FILL = 1000\n",
        "    with tf.Session() as sess:\n",
        "        np.random.seed(RANDOM_SEED)\n",
        "        tf.set_random_seed(RANDOM_SEED)\n",
        "        env = gym.make(ENV_NAME)\n",
        "        state_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.shape[0]\n",
        "        low = TD3(sess, state_dim, action_dim, max_action, min_action, ACTOR_LEARNING_RATE, CRITIC_LEARNING_RATE, TAU, RANDOM_SEED, device=DEVICE)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        # check that we are effectively updating the parameters\n",
        "        #print(low.a_params_target[0].eval()[0][0],low.a_params[0].eval()[0][0])\n",
        "        #low.update_target_network()\n",
        "        #print(low.a_params_target[0].eval()[0][0],low.a_params[0].eval()[0][0])\n",
        "        replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
        "        ruido = OUNoise(action_dim, mu = 0.0)\n",
        "        for i in range(epochs):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            epsilon -= (epsilon/EXPLORE)\n",
        "            epsilon = np.maximum(min_epsilon,epsilon)\n",
        "            episode_r = 0.\n",
        "            step = 0\n",
        "            while (not done):\n",
        "                step += 1\n",
        "                action = low.predict_action(np.reshape(state,(1,state_dim)))\n",
        "                action1 = action\n",
        "                action = np.clip(action,min_action,max_action)\n",
        "                action = action + max(epsilon,0)*ruido.noise()\n",
        "                action = np.clip(action,min_action,max_action)\n",
        "                # print(action1, action)\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                reward = reward + 1. \n",
        "                # reward = np.clip(reward,-1.,1.)\n",
        "                replay_buffer.add(np.reshape(state, (state_dim,)), np.reshape(action, (action_dim,)), reward,\n",
        "                                      done, np.reshape(next_state, (state_dim,)))\n",
        "                state = next_state\n",
        "                episode_r = episode_r + reward\n",
        "                if replay_buffer.size() > MINIBATCH_SIZE + FILL:\n",
        "                    s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(MINIBATCH_SIZE)\n",
        "                    low.train(s_batch, a_batch, r_batch, t_batch, s2_batch,MINIBATCH_SIZE)\n",
        "                    #low.train_normal(s_batch, a_batch, r_batch, t_batch, s2_batch,MINIBATCH_SIZE)\n",
        "                else: \n",
        "                    epsilon  = 1.\n",
        "            print(i, step, 'last r', round(reward,3), 'episode reward',round(episode_r,3), 'epsilon', round(epsilon,3))                \n",
        "\n",
        "\n",
        "        low.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0214e189ea32>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    tf.set_random_seed(RANDOM_SEED)\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    }
  ]
}